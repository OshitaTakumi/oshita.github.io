<!DOCTYPE html>
<html lang="ja">
<head>
<!-- Mathjaxの設定．これで数式のインライン形式が生成可能 -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
	MathJax.Hub.Config({
	  tex2jax: {
		inlineMath: [['$','$']]
	  }
	});
</script>

<meta charset="UTF-8">
<title>Implicit and Explicit Commonsense for Multi-sentence Video Captioning 要約</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="description" content="Implicit and Explicit Commonsense for Multi-sentence Video Captioning 要約">
<link rel="stylesheet" href="../../../css/style.css">
</head>

<body>

<div id="container">

<header>

<div id="logo">
<h1><a href="index.html"><img src="images/logo.png" alt="Visual prompt"></a></h1>
</div>

</header>

<!--開閉ブロック-->
<div id="menubar">

<nav>
	<ul>
	<li><a href="index.html">ホーム</a></li>
	<li><a href="selh-intro.html">自己紹介</a></li>
	<li><a href="">過去の研究</a>
		<ul>
			<li><a href="../../../my_app.html">個人製作アプリ</a></li>
			<li><a href="../../../b3.html">学部３年</a></li>
			<li><a href="../../../b4.html">学部４年</a></li>
			<li><a href="../../../404.html">修士１年</a></li>
			<li><a href="../../../404.html">修士２年</a></li>
		</ul>
	</li>
	<li><a href="club.html">学外活動</a></li>
	<li><a href="contact.html">お問い合わせ</a></li>
	</ul>
</nav>

<!-- <div class="sh">
<p>※800px以下のメニュー開閉時にのみ表示させたい情報があればここ（shボックスの中）に入れて下さい。<br>
サンプルテキスト。サンプルテキスト。<br>
サンプルテキスト。サンプルテキスト。<br>
サンプルテキスト。サンプルテキスト。</p>
</div> -->
<!--/.sh-->

</div>
<!--/#menubar-->

<main>

<section>

<h2>論文</h2>
<p>・ Year ： ICLR2023<br>
	・ <a href="https://arxiv.org/abs/2211.14296">リンク</a><br> 
	・ <a href="https://github.com/frt03/mxt_bench">github</a>
</p>

<h2>１．どんなものか</h2>
<p>
	既存のビデオキャプションアプローチはビデオの全体的な表現に依存．これらの学習法では自称の因果関係や
	特定のオブジェクトの機能等を推論するのに必要な世界に関する常識的な知識が根本的にかけている．これに
	対してこの論文では
	<ul>
		<li>
			暗黙的な常識知...視覚言語的、純粋言語的な要素
		</li>
		<li>
			明示的な常識知識...知識ベースな要素
		</li>
	</ul>
	の両者について考慮した，transforerベースのビデオキャプションモデルを提案している．<br>
	また模倣学習からヒントを得て，「命令生成タスク」という新しい拓くを提案している．
	常識を知識として活用することで既存のビデオキャプションアプローチが既存研究に対して大幅
	な改善をもたらした．
	<image class='image' src="./caption flow.png">
</p>

<h2>２．先行研究との差分</h2>
<p>
	過去の研究例では，階層型デコーダや記憶増強型ネットワークを用いて学習がなされている．しかしこれらはすべて
	ビデオとキャプションから直接学習されており，既存の大規模モデルからもたらされるような常識，因果関係についての概念がない．<br>
	<h3>コモセンス推論</h3>
	<p>
		コモセンス推論と言語モデリングはNLPタスクで広く使用される．
		<ul>
			<li>
				ConceptNet...分類学的・語彙的知識(related to, synonym, is -Aなど)と物理的コモセンス知識(made of, part of)に焦点を当てている．
			</li>

			<li>
				ATOMIC...ATOMICの知識グラフはxIntent,xWant,oWantなどをカバーする9つの関係にわたり，1.33Mのタプルを含んでいる．
			</li>
		</ul>
	しかしこれらは未知のデータに対して常識的知識を絡めた推論ができない．そのため結果としては未知データに汎化できない．<br>
	これに対してCOMETはKBsと呼ばれる言語モデルをfine-tuningすることで未知データに対する常識に関係した推論を生成可能．
	この研究では，複数文の生成においてＣＯＭＥＴを明示的な常識事前知識として用いている．
	</p>
</p>

<h2>３．技術や研究のキモ</h2>
<p>
	※事前知識：動画に対してキャプション付け：ビデオの長さ，キャプション付けを行うビデオセグメントの決定が必要．<br>
	明示的，暗黙的の２種類の常識知識をモデルに統合する．<br>
		<image class='image' src="./video_caption_model.png">
	<h3>明示的知識の抽出</h3>
	<p>
		必要なのはスニペットとその前後のスニペットとの対応関係．BARTを用いたCOMETを使用して明示的知識を基盤モデルから抽出．
		これをSBERTによってエンコードする，すべて事前学習済み．
	</p>
	<h3>暗黙的知識の抽出</h3>
	<p>
		The Pileデータセットで事前学習されたGPT-Neoを使用．GPTの出力を暗黙的事前知識として，入力に対して出力された行動予測文章を
		SBERTによりエンコード
	</p>
	<h3>アクション-オブジェクト予測器</h3>
	<p>
		動画のスニペットは前後で強い関係性を持つはずである．そこで行動・物体予測器$fa$を用いる．$fa$では動画中のオブジェクトに対して
		CLIPで検出．データセットに対して行動が与えられていることが前提．前のスニペットとの情報量に対してはMultiHeadAttensionを行うことで解消．
		以下のように定式化可能．ただしnを2値のスニペットを表すとする．
		$$
			V_i = V \oplus \bm{n}_i
		$$
		$$
			\bm{a}_i = f_a{concat(V_i,\bm{m}_{i-1},\bm{g}_{i-1},\bm{h}_{i-1})}
		$$
		$$
			L_{actobj} = L_{}
		$$
		$$
			V_e^{n} = V_e^{(n-1)} + MultiHead(V_e^{(n-1)},V_e^{(n-1)},V_e^{(n-1)}) 
		$$
		<image class='image' src="./detail.png"></image>
	</p>

</p>



<h2>４．有効性の証明</h2>
<p>
	Alfred,ActivityNet Captionsを用いて実験． また，動画の種類について定義．
	<ul>
		<li>ParagraphLevel：スニペットの推定や監視を行わない（潜在的なまま）設定</li>
		<li>Sentence-Level: スニペットを推定し、密なビデオキャプションと同様にキャプションに活用することを学習するもの</li>
	</ul>

	ALFREDデータセットは、行動、振る舞い、物体を人間の言語と結びつけるためのベンチマーク.
	このデータセットはAI2-THOR 2.0上の対話型視覚環境で構築されている．<br>
	ActivityNet Captions Datasetは各動画は時間的に注釈された一連の文章を持ち、各文章は発生したイベントを記
	述する動画のセグメントに対応する．<br>
	生成した命令文について実験，評価手法には，BLEU, METEOR,CIDErを用いて評価．Alfredに結果は以下の通り．<br>
	<image class="'image" src="./alfred_result.png"><br>

	<h3>密なビデオキャプションにおける実験結果</h3>
	一般性を説明するために、我々はまた、密なビデオキャプション問題の設定において、ActivityNet Captionsデータセットで
	提案手法の方法の性能を実証した。以下に定性的な実験結果，生成された命令文の評価実験結果を示す<br>
	<image class='image' src="./activitynet_result.png">
	<image class='image' src="./activitynet_centence_result.png">
	
	<h3>常識知識に関するユーザー実験</h3>
	常識知識の改善を定量化することを目的とする実験．密なビデオキャプションの設定において文章AかBのどちらがいいか，それとも両者ともいい，
	もしくは悪いかについて答えてもらった．指示生成では10件、密なビデオキャプションでは12件の回答を収集．インストラクション生成では82.6%、
	dense video captioningでは54.5%の文章が、常識知識を加えたモデルの文章と同等かそれ以上であると判定された．
	
</p>

<h2>５．議論</h2>	
<p>
	特になし
</p>


<h2>感想</h2>	
<p>
	暗黙的，明示的知識を事前学習済モデルから抽出して，キャプション付けに対して汎化できた例．
	CLIPが対応していないようなドメインの画像についてはCLIPのfinu-tuningで対処可能なのかは
	たまた別のアプローチが必要なのかについては気になるところ．
</p>


</section>


</main>

<footer>


<div class="copy">
<small>Copyright&copy; <a href="index.html">SAMPLE COMPANY</a> All Rights Reserved.</small>
<span class="pr"><a href="https://template-party.com/" target="_blank">《Web Design:Template-Party》</a></span>
</div>

</footer>

<!--開閉ボタン（ハンバーガーアイコン）-->
<div id="menubar_hdr">
<span></span><span></span><span></span>
</div>

</div>
<!--/#container-->

<!--jQueryの読み込み-->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>

<!--パララックス（inview）-->
<script src="https://cdnjs.cloudflare.com/ajax/libs/protonet-jquery.inview/1.1.2/jquery.inview.min.js"></script>
<script src="js/jquery.inview_set.js"></script>

<!--このテンプレート専用のスクリプト-->
<script src="js/main.js"></script>

<!--ページの上部へ戻るボタン-->
<div class="pagetop"><a href="#"><i class="fas fa-angle-double-up"></i></a></div>

</body>
</html>
