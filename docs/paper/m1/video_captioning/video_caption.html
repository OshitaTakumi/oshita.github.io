<!DOCTYPE html>
<html lang="ja">
<head>
<!-- Mathjaxの設定．これで数式のインライン形式が生成可能 -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
	MathJax.Hub.Config({
	  tex2jax: {
		inlineMath: [['$','$']]
	  }
	});
</script>

<meta charset="UTF-8">
<title>Implicit and Explicit Commonsense for Multi-sentence Video Captioning 要約</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="description" content="Implicit and Explicit Commonsense for Multi-sentence Video Captioning 要約">
<link rel="stylesheet" href="../../../css/style.css">
</head>

<body>

<div id="container">

<header>

<div id="logo">
<h1><a href="index.html"><img src="images/logo.png" alt="Visual prompt"></a></h1>
</div>

</header>

<!--開閉ブロック-->
<div id="menubar">

<nav>
	<ul>
	<li><a href="index.html">ホーム</a></li>
	<li><a href="selh-intro.html">自己紹介</a></li>
	<li><a href="">過去の研究</a>
		<ul>
			<li><a href="../../../my_app.html">個人製作アプリ</a></li>
			<li><a href="../../../b3.html">学部３年</a></li>
			<li><a href="../../../b4.html">学部４年</a></li>
			<li><a href="../../../404.html">修士１年</a></li>
			<li><a href="../../../404.html">修士２年</a></li>
		</ul>
	</li>
	<li><a href="club.html">学外活動</a></li>
	<li><a href="contact.html">お問い合わせ</a></li>
	</ul>
</nav>

<!-- <div class="sh">
<p>※800px以下のメニュー開閉時にのみ表示させたい情報があればここ（shボックスの中）に入れて下さい。<br>
サンプルテキスト。サンプルテキスト。<br>
サンプルテキスト。サンプルテキスト。<br>
サンプルテキスト。サンプルテキスト。</p>
</div> -->
<!--/.sh-->

</div>
<!--/#menubar-->

<main>

<section>

<h2>論文</h2>
<p>・ Year ： ICLR2023<br>
	・ <a href="https://arxiv.org/abs/2211.14296">リンク</a><br> 
	・ <a href="https://github.com/frt03/mxt_bench">github</a>
</p>

<h2>１．どんなものか</h2>
<p>
	既存のビデオキャプションアプローチはビデオの全体的な表現に依存．これらの学習法では自称の因果関係や
	特定のオブジェクトの機能等を推論するのに必要な世界に関する常識的な知識が根本的にかけている．これに
	対してこの論文では
	<ul>
		<li>
			暗黙的な常識知...視覚言語的、純粋言語的な要素
		</li>
		<li>
			明示的な常識知識...知識ベースな要素
		</li>
	</ul>
	の両者について考慮した，transforerベースのビデオキャプションモデルを提案している．<br>
	また模倣学習からヒントを得て，「命令生成タスク」という新しい拓くを提案している．
	常識を知識として活用することで既存のビデオキャプションアプローチが既存研究に対して大幅
	な改善をもたらした．
	<image class='image' src="./caption flow.png">
</p>

<h2>２．先行研究との差分</h2>
<p>
	過去の研究例では，階層型デコーダや記憶増強型ネットワークを用いて学習がなされている．しかしこれらはすべて
	ビデオとキャプションから直接学習されており，既存の大規模モデルからもたらされるような常識，因果関係についての概念がない．<br>
	<h3>コモセンス推論</h3>
	<p>
		コモセンス推論と言語モデリングはNLPタスクで広く使用される．
		<ul>
			<li>
				ConceptNet...分類学的・語彙的知識(related to, synonym, is -Aなど)と物理的コモセンス知識(made of, part of)に焦点を当てている．
			</li>

			<li>
				ATOMIC...ATOMICの知識グラフはxIntent,xWant,oWantなどをカバーする9つの関係にわたり，1.33Mのタプルを含んでいる．
			</li>
		</ul>
	しかしこれらは未知のデータに対して常識的知識を絡めた推論ができない．そのため結果としては未知データに汎化できない．<br>
	これに対してCOMETはKBsと呼ばれる言語モデルをfine-tuningすることで未知データに対する常識に関係した推論を生成可能．
	この研究では，複数文の生成においてＣＯＭＥＴを明示的な常識事前知識として用いている．
	</p>
</p>

<h2>３．技術や研究のキモ</h2>
<p>
	※事前知識：動画に対してキャプション付け：ビデオの長さ，キャプション付けを行うビデオセグメントの決定が必要．<br>
	明示的，暗黙的の２種類の常識知識をモデルに統合する．<br>
		<image class='image' src="./video_caption_model.png">
	<h3>明示的知識の抽出</h3>
	<p>
		必要なのはスニペットとその前後のスニペットとの対応関係．BARTを用いたCOMETを使用して明示的知識を基盤モデルから抽出．
		これをSBERTによってエンコードする，すべて事前学習済み．
	</p>
	<h3>暗黙的知識の抽出</h3>
	<p>
		The Pileデータセットで事前学習されたGPT-Neoを使用．GPTの出力を暗黙的事前知識として，入力に対して出力された行動予測文章を
		SBERTによりエンコード
	</p>
	<h3>アクション-オブジェクト予測器</h3>
	<p>
		動画のスニペットは前後で強い関係性を持つはずである．そこで行動・物体予測器$fa$を用いる．$fa$では動画中のオブジェクトに対して
		CLIPで検出．データセットに対して行動が与えられていることが前提．以下のように定式化可能．ただしnを2値のスニペットを表すとする
		$$
			V_i = V \oplus \bm{n}_i
		$$
		$$
			\bm{a}_i = f_a{concat(V_i,\bm{m}_{i-1},\bm{g}_{i-1},\bm{h}_{i-1})}
		$$
		$$
			L_{actobj} = L_{}
		$$


	</p>

</p>



<h2>４．有効性の証明</h2>
キーポイントの命名、キーポイントの定位、および表現理解の参照に
ついて考え、VLMにおける視覚的マーキングの特性を研究．<br>
<h3>キーポイントの命名</h3>
CUB-200-2011とSPair71kデータセットを使用．CUB-200-2011は各画像のキーポイントに対して
名前を付けた注釈を含んでいるが，後者は画像のペアで共通する部分についての情報は持っているが
名前については持っていない．この実験ではこれに対して手動で名前を付けている．<br>
以下の実験結果では，キーポイントの命名について性能向上が確認され，また画像に対する円の書き方についても
赤丸が最も良いということが示された．人間が赤丸を用いて注釈を用いる傾向にあることが可能性として考えられる．
<image class='image' src="./spair71k.png"><br>
<image class='image' src="./spair_hand_labeled.png"><br>
<image class='image' src="./naming_result.png"><br>
<image class='image' src="./prompt_way.png"><br>	

<h3>CLIPの学習データ上のマーカーを含むデータについて</h3>
CLIPが赤丸の付いたデータを学習したことにより，ゼロショットでもこのような現象が起きているのではないかについて調査．
YFCC15MというCLIPの学習に用いられたデータのサブセットからこのようなデータを実際に探した．
分類器に対してvit_b_16，resnet50x16のCLIPビジョンエンコーダのアンサンブルを用いた2クラス分類を解かせることで，YFCC15M
の6Mサブセットから，スコアの高い順に10kの画像を取得し，最後はこの1万枚の画像を手動で調べることでアノテーションの書かれた画像を
70枚獲得した．以下実例<br>
<image class='image' src="./annotationed_traindata.png"><br>
このことからもマーカー付きのデータは非常にまれ(0.001%ほど)であり，この挙動自体も大容量モデルが大規模データセットからでし学習できない
ことが示唆される．異なる事前学習データセットにおけるCLIPの性能比較もなされており，一般的に，キーポイントマッチングの性能は事前学習データ
セットサイズ，vitのエンコーダサイズに依存する．その結果が次の通り．<br>
<image class='image' src="./naming_key_point.png"><br>
データ数が多いLAION-2Bの事前学習結果の方が性能が悪いのはWIT-400MとLAION-2Bの作成時のフィルタリングの違いによるもので、後者では、生成モデルの美的
感覚を重視したため、アノテーションの例が削除された可能性があると論文では推測している．データ数，エンコーダサイズに従って性能は良い傾向にあるが，
そのまま収束するとはならず，WIT-400Mで事前学習したCLIPの性能が劇的に向上したことが見受けられる．<br>
<image class='image' src="./zs_result.png">

<h3>キーポイントローカリゼーション</h3>
<p>
メトリックとして正しいキーポイントの割合（PCK）<br>
ここで$\delta$は$\delta=\alpha max(H,W)$で与えられる距離閾値であり,$\alpha$は0~1．(H,W)はBBサイズ．
実験結果より，ゼロショットのみならず弱教師を用いる既存手法に対しても有意差あり．データセット次第ではあるが
zeroショットから更にも一歩を踏み出せる場合がある．<br>
<image class='image' src="./pck.png"><br>
<image src="./localize_leypoint.png" class="image">
</p>

<h3>参照表現の理解</h3>
RefCOCO,RefCOCO+,RefCOCOgの３つのデータセットで実験．
各々，COCOとその一意のオブジェクトを参照する表現からなりBBもついている．
RefCOCO+は外観に関する表現を含んでおり，RefCOCO,RefCOCOgは物体の関係性を参照表現に含む．
RefCOCOとRefCOCO+のテストセットは2つに分割されており、testAとtestBにはそれぞれ人物と非人間のみ
含まれる．
<image src="./rec.png" class="image"></image>

<h2>５．議論</h2>	
<p>
	画像に対して赤丸を追加するだけでVLMからオブジェクトに対する有益な情報を獲得できるが，問題点も存在する．
	COCOにおいて赤丸を付けた画像に対して，｛男性，女性，行方不明者，殺人容疑者｝のワードに対してスコアリングすると
	zsに比べて行方不明者，殺人容疑者のスコアが上昇する傾向にある．
</p>


<h2>感想</h2>	
<p>

</p>


</section>


</main>

<footer>


<div class="copy">
<small>Copyright&copy; <a href="index.html">SAMPLE COMPANY</a> All Rights Reserved.</small>
<span class="pr"><a href="https://template-party.com/" target="_blank">《Web Design:Template-Party》</a></span>
</div>

</footer>

<!--開閉ボタン（ハンバーガーアイコン）-->
<div id="menubar_hdr">
<span></span><span></span><span></span>
</div>

</div>
<!--/#container-->

<!--jQueryの読み込み-->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>

<!--パララックス（inview）-->
<script src="https://cdnjs.cloudflare.com/ajax/libs/protonet-jquery.inview/1.1.2/jquery.inview.min.js"></script>
<script src="js/jquery.inview_set.js"></script>

<!--このテンプレート専用のスクリプト-->
<script src="js/main.js"></script>

<!--ページの上部へ戻るボタン-->
<div class="pagetop"><a href="#"><i class="fas fa-angle-double-up"></i></a></div>

</body>
</html>
