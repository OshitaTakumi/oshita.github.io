<!DOCTYPE html>
<html lang="ja">
<head>
<!-- Mathjaxの設定．これで数式のインライン形式が生成可能 -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
	MathJax.Hub.Config({
	  tex2jax: {
		inlineMath: [['$','$']]
	  }
	});
</script>

<meta charset="UTF-8">
<title>What does CLIP know about a red circle? Visual prompt engineering for VLMs 要約</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="description" content="What does CLIP know about a red circle? Visual prompt engineering for VLMs 要約">
<link rel="stylesheet" href="../../../css/style.css">
</head>

<body>

<div id="container">

<header>

<div id="logo">
<h1><a href="index.html"><img src="images/logo.png" alt="Visual prompt"></a></h1>
</div>

</header>

<!--開閉ブロック-->
<div id="menubar">

<nav>
	<ul>
	<li><a href="index.html">ホーム</a></li>
	<li><a href="selh-intro.html">自己紹介</a></li>
	<li><a href="">過去の研究</a>
		<ul>
			<li><a href="../../../my_app.html">個人製作アプリ</a></li>
			<li><a href="../../../b3.html">学部３年</a></li>
			<li><a href="../../../b4.html">学部４年</a></li>
			<li><a href="../../../404.html">修士１年</a></li>
			<li><a href="../../../404.html">修士２年</a></li>
		</ul>
	</li>
	<li><a href="club.html">学外活動</a></li>
	<li><a href="contact.html">お問い合わせ</a></li>
	</ul>
</nav>

<!-- <div class="sh">
<p>※800px以下のメニュー開閉時にのみ表示させたい情報があればここ（shボックスの中）に入れて下さい。<br>
サンプルテキスト。サンプルテキスト。<br>
サンプルテキスト。サンプルテキスト。<br>
サンプルテキスト。サンプルテキスト。</p>
</div> -->
<!--/.sh-->

</div>
<!--/#menubar-->

<main>

<section>

<h2>論文</h2>
<p>・ Year ： 2023<br>
	・ <a href="https://arxiv.org/abs/2304.06712">リンク</a><br> 
	・ <a href="">github</a>
</p>

<h2>１．どんなものか</h2>
<p>
	CLIPには画像に対して円を書くだけでグローバルな画像情報を保ちつつ，円部分に対してモデルの注意を向けることができる
	性質がある．
	目的は2つある．<br>
	1つ目はゼロショットでVLM(大規模な視覚言語モデル)から有益な情報を抽出するためのツールの作成<br>
	2つ目はVLMとその訓練データの興味深く予想外の特性を明らかにすること，具体的には画像中に円を記すことで
	会話の内容をオブジェクトに寄せる，画像領域の分析といった視覚的プロンプトエンジニアリングについての調査である<br>
	<image class='image' src="./img1.png">
</p>

<h2>２．先行研究との差分</h2>
<p>
	参照表現理解(reffering expression comprehension)についてはCLIPの登場により可能になったがそれらの研究は
	あまり進んでいない．<br>
	ReCLIPは左／右、小さい／大きいなどの関係を考慮するためのアドホックな後処理ステップの前に、CLIPを使用して
	オブジェクト提案を切り取り、それらをランク付けする。<br>
	CPTは、オブジェクト提案のボックスに色を付け、事前学習されたキャプションモデルを使用して、どの色の提案が
	クエリ記述に対応するかを自動回帰的に予測する．<br>
	人気な研究になっていくRECにではあるものの，VLMの偏りを評価する方法は、まだ十分に確立されていない．
	下流タスクに対してゼロショットで適用する場合はこれは非常に重要．
</p>

<h2>３．技術や研究のキモ</h2>
<p>
	一般的には，VLMでは画像とテキスト間のスコアを評価するが，質問と解答をそのスコアと評価することはできない．
	しかし，プロンプトエンジニアリングによりこれが可能になった．具体的には入力画像とテキストのペアを条件として
	質問と回答の互換性スコア  s(question,answer|image,text)を構築することが可能になった．これは一般的に以下のように定式化される．
	$$
		s(q,a|i,t) = s(iqa,tqa)
	$$
	ここでiqaとtqaは入力画像とテキストのバージョンで、質問と回答のペア（q、a）を反映するように後者を変換することによって得られる．
	<image src="./img2.png" class='image'>
</p>

<h3>名前付きキーポイントローカライズ問題</h3>

<h4>テキスト側のプロンプトエンジニアリング</h4>
<p>
	キャプションが「犬」の入力画像iに対して，質問q='右耳'に答えるような問題．
	これに対して tqa = tq = 'an image of the right ear of a dog' を設定
	することにより，実在物の説明をエンコード可能．
</p>

<h4>入力画像側のプロンプトエンジニアリング</h4>
<p>
	マーキングによるvisual promptingは参照表現の解釈など，VLMでは幅広く用いられている．
	$s(ia,tq)$を最大化することによって参照表現tqに対して最も一致する画像の切り出しを
	求めることができる．<br>,
	この論文の「マーキング」は画像の切り出しではなく，文字通り画像中に円，または矢印を視覚的に
	示す．マーキングされた画像iaのぐらーばるな情報た維持しつつ，アテンションがマーキング部分に
	集中する．マーキングはVLMと相性がよく，いくつかの予測タスクにおいては切り抜きベースの
	プロンプトエンジニアリングを凌駕する．
</p>

<h3>マーキングベースプロンプトエンジニアリング</h3>
キーポイントと名前の一致から参照表現理解までの様々な難易度のタスクについて
ゼロショット予測を考慮し，マークベースのプロンプトエンジニアリングについて研究
<h4>キーポイントの命名</h4>
<p>
	最も単純なタスクとしてはオブジェクトのキーポイントの名前と画像内の２次元的
	な位置とのマッチングが挙げられる．画像をi，キーポイント名をQ，対応する位置
	$A\in{0,...,H-1} \times {0,...,W-1}$とする．各画像に対して質問とそれが
	表す画像内の位置が一致することが望ましい．式は次の通り．$m=|Q|=|A|$　<br>
	詳細としては，各名前Qに対してAを関連づけるような正方順序行列$\prod \in S_m$
	を予測する．ここで$\prod_{qa}=1$<br>
	ここで予測にはコスト関数を$C_{qa}=s(i_a,t_q)$で定義して，最適輸送距離を用いた
	以下の式で計算する．<br>
	<image src="./wasserstain.png" class='image'><br>
</p>

<h4>キーポイントのローカライズ</h4>
<p>
	目的：画像中のキーポイントqの特定
	キーポイントの命名と異なる点はキーポイントに関する位置所法の事前知識を仮定しないこと．
	つまり$A\in{0,...,H-1} \times {0,...,W-1}$が事前情報として獲得できない．その中から領域
	$m \times m$を推測する．キーポイント名qが与えられた場合，その位置aは
	$$
	\hat{a}(i,q) = \underset{argmax{a \in A}} 
	[s(i_a,t_q)-\frac{1}{|Q|}\sum_{\overline{q}\in Q}s(i_s,t_{\overline{q}})]$$
	で与えられる．最も質問とそれに対するスコアが高かった領域aがローカライズされる．
	ここで意味のない領域を抽出しないように<a href="https://arxiv.org/abs/2209.00383">先行研究</a>
	に従いサリエンシー法によって制限される．
</p>

<h4>参照表現の理解</h4>
<p>
	参照表現の理解：画像中のオブジェクトに対して，それを明示的に参照するテキストに対応
	するものとして検出すること．例えば「右から4番目の犬」のような表現に対して正しく入力画像
	の右から４番目の犬が検出されればモデルは理解しているということ．
	アプローチ：<a href="https://arxiv.org/abs/2203.08481">先行研究</a>で示されるように
	画像iに対して<a href="https://arxiv.org/abs/1801.08186">既存手法</a>を用いてオブジェクト
	と提案のセットを抽出し，可能な答えセットAとして解釈．これを以下のように定式化．<br>
	<image src="./object_proposal.png" class='image'></image>
	
</p>

<h2>４．有効性の証明</h2>
キーポイントの命名、キーポイントの定位、および表現理解の参照に
ついて考え、VLMにおける視覚的マーキングの特性を研究．<br>
<h3>キーポイントの命名</h3>
CUB-200-2011とSPair71kデータセットを使用．CUB-200-2011は各画像のキーポイントに対して
名前を付けた注釈を含んでいるが，後者は画像のペアで共通する部分についての情報は持っているが
名前については持っていない．この実験ではこれに対して手動で名前を付けている．<br>
以下の実験結果では，キーポイントの命名について性能向上が確認され，また画像に対する円の書き方についても
赤丸が最も良いということが示された．人間が赤丸を用いて注釈を用いる傾向にあることが可能性として考えられる．
<image class='image' src="./spair71k.png"><br>
<image class='image' src="./spair_hand_labeled.png"><br>
<image class='image' src="./naming_result.png"><br>
<image class='image' src="./prompt_way.png"><br>	

<h3>CLIPの学習データ上のマーカーを含むデータについて</h3>
CLIPが赤丸の付いたデータを学習したことにより，ゼロショットでもこのような現象が起きているのではないかについて調査．
YFCC15MというCLIPの学習に用いられたデータのサブセットからこのようなデータを実際に探した．
分類器に対してvit_b_16，resnet50x16のCLIPビジョンエンコーダのアンサンブルを用いた2クラス分類を解かせることで，YFCC15M
の6Mサブセットから，スコアの高い順に10kの画像を取得し，最後はこの1万枚の画像を手動で調べることでアノテーションの書かれた画像を
70枚獲得した．以下実例<br>
<image class='image' src="./annotationed_traindata.png"><br>
このことからもマーカー付きのデータは非常にまれ(0.001%ほど)であり，この挙動自体も大容量モデルが大規模データセットからでし学習できない
ことが示唆される．異なる事前学習データセットにおけるCLIPの性能比較もなされており，一般的に，キーポイントマッチングの性能は事前学習データ
セットサイズ，vitのエンコーダサイズに依存する．その結果が次の通り．<br>
<image class='image' src="./naming_key_point.png"><br>
データ数が多いLAION-2Bの事前学習結果の方が性能が悪いのはWIT-400MとLAION-2Bの作成時のフィルタリングの違いによるもので、後者では、生成モデルの美的
感覚を重視したため、アノテーションの例が削除された可能性があると論文では推測している．データ数，エンコーダサイズに従って性能は良い傾向にあるが，
そのまま収束するとはならず，WIT-400Mで事前学習したCLIPの性能が劇的に向上したことが見受けられる．<br>
<image class='image' src="./zs_result.png">

<h3>キーポイントローカリゼーション</h3>
<p>
メトリックとして正しいキーポイントの割合（PCK）<br>
ここで$\delta$は$\delta=\alpha max(H,W)$で与えられる距離閾値であり,$\alpha$は0~1．(H,W)はBBサイズ．
実験結果より，ゼロショットのみならず弱教師を用いる既存手法に対しても有意差あり．データセット次第ではあるが
zeroショットから更にも一歩を踏み出せる場合がある．<br>
<image class='image' src="./pck.png"><br>
<image src="./localize_leypoint.png" class="image">
</p>

<h3>参照表現の理解</h3>
RefCOCO,RefCOCO+,RefCOCOgの３つのデータセットで実験．
各々，COCOとその一意のオブジェクトを参照する表現からなりBBもついている．
RefCOCO+は外観に関する表現を含んでおり，RefCOCO,RefCOCOgは物体の関係性を参照表現に含む．
RefCOCOとRefCOCO+のテストセットは2つに分割されており、testAとtestBにはそれぞれ人物と非人間のみ
含まれる．
<image src="./rec.png" class="image"></image>

<h2>５．議論</h2>	
<p>
	画像に対して赤丸を追加するだけでVLMからオブジェクトに対する有益な情報を獲得できるが，問題点も存在する．
	COCOにおいて赤丸を付けた画像に対して，｛男性，女性，行方不明者，殺人容疑者｝のワードに対してスコアリングすると
	zsに比べて行方不明者，殺人容疑者のスコアが上昇する傾向にある．
</p>


<h2>感想</h2>	
<p>

</p>


</section>


</main>

<footer>


<div class="copy">
<small>Copyright&copy; <a href="index.html">SAMPLE COMPANY</a> All Rights Reserved.</small>
<span class="pr"><a href="https://template-party.com/" target="_blank">《Web Design:Template-Party》</a></span>
</div>

</footer>

<!--開閉ボタン（ハンバーガーアイコン）-->
<div id="menubar_hdr">
<span></span><span></span><span></span>
</div>

</div>
<!--/#container-->

<!--jQueryの読み込み-->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>

<!--パララックス（inview）-->
<script src="https://cdnjs.cloudflare.com/ajax/libs/protonet-jquery.inview/1.1.2/jquery.inview.min.js"></script>
<script src="js/jquery.inview_set.js"></script>

<!--このテンプレート専用のスクリプト-->
<script src="js/main.js"></script>

<!--ページの上部へ戻るボタン-->
<div class="pagetop"><a href="#"><i class="fas fa-angle-double-up"></i></a></div>

</body>
</html>
